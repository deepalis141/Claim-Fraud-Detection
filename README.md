# Claim-Fraud-Detection
Machine learning project to detect fraudulent insurance claims using classification models, SMOTE imbalance handling, Isolation Forest anomaly detection, business rules, and hybrid fraud scoring. Includes Tableau dashboard and full end-to-end pipeline.




# ğŸ•µï¸ Claim Fraud Detection

### **Machine Learning + Isolation Forest + Rules Engine + Tableau Dashboard**

---

## ğŸ“Œ **Objective**

Detect **fraudulent or suspicious insurance claims** using:

* Classification models (Logistic Regression, Random Forest)
* Imbalanced learning techniques (class weights, SMOTE)
* Anomaly detection (Isolation Forest)
* Business rule scoring
* Hybrid fraud scoring
* Tableau dashboard for risk visualization

This project simulates a real insurance fraud analytics workflow.

---

## ğŸ“‚ **Project Structure**

```
fraud_detection_project/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ creditcard.csv                 # dataset downloaded from Kaggle
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_fraud_eda_cleaning.ipynb    # EDA + preprocessing
â”‚   â”œâ”€â”€ 02_fraud_modeling.ipynb        # ML models + Isolation Forest + scoring
â”‚   â””â”€â”€ 03_fraud_tableau_export.ipynb  # output CSV for Tableau dashboard
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ fraud_rf_model.pkl             # trained RandomForest model
â”‚   â””â”€â”€ fraud_scaler.pkl               # StandardScaler for preprocessing
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ fraud_dashboard_data.csv       # final dataset for Tableau
â”‚
â””â”€â”€ README.md
```

---

## ğŸ“Š **Dataset**

For this project, the **Credit Card Fraud Detection** dataset is used, as it closely mimics fraud patterns:

* Highly imbalanced (fraud â‰ˆ 0.17%)
* Transaction features (V1â€“V28 from PCA)
* Amount, Time, Class (0 = normal, 1 = fraud)

ğŸ“¥ Kaggle link:
[https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud)

> In a real insurance dataset, features would include policy age, claim history, severity, repair garage flags, etc.
> The modeling approach here is identical.

---

## ğŸ§¹ **1. Data Cleaning & Fraud EDA (Notebook 1)**

Performed in `01_fraud_eda_cleaning.ipynb`:

### âœ” Loaded and inspected dataset

* Checked missing values
* Verified fraud imbalance
* Visualized:

  * **class distribution**
  * **transaction amount vs fraud**
  * **time-of-day fraud patterns**

### âœ” Saved processed dataset

Output:
`data/fraud_processed.csv`

---

## ğŸ§  **2. Feature Engineering**

Applied inside Notebook 2:

âœ” Scaled numerical features with **StandardScaler**
âœ” Created domain-inspired "rule features" such as:

* high amount indicator
* early-day transaction indicator

> In real insurance datasets you would add:
>
> * policy age
> * number of past claims
> * severity ratio
> * garage/hospital flags
> * accident type indicators

---

## ğŸ¤– **3. Machine Learning Models (Classification)**

Trained 3 models:

### ğŸ”¹ Logistic Regression

* Used `class_weight="balanced"`
* Handles imbalance
* Good baseline model

### ğŸ”¹ Random Forest

* `class_weight="balanced"`
* More robust than Logistic Regression
* Higher recall on fraud class

### ğŸ”¹ Random Forest + SMOTE

* Oversampled fraud cases
* Best balance between precision/recall
* **Selected as primary ML model**

### ğŸ”¹ Evaluation Metrics

* Precision
* Recall
* F1-score
* ROCâ€“AUC

---

## ğŸ§© **4. Imbalanced Data Handling**

Fraud datasets are extremely skewed.

We applied:

### âœ” Class weights

(LR & RF trained with `class_weight="balanced"`)

### âœ” SMOTE Oversampling

* Generates synthetic fraud examples
* Improved fraud recall significantly

---

## ğŸš¨ **5. Isolation Forest (Anomaly Detection)**

An unsupervised model trained to detect unusual transactions.

* Flags anomalies with `-1`
* Combines well with supervised ML
* Gives anomaly score (`iso_anomaly_score`)
* Helps catch unseen fraud patterns

---

## ğŸ§® **6. Business Rules Engine**

A simple rule-based system:

### Example rules used:

* High transaction amount
* Transaction in early window (first hour)

Rules generate a **rule_score** between 0 and 1.

> In actual insurance datasets, rules include:
> duplicate claims, mismatched documents, abnormal repair costs, clinic red flags, etc.

---

## ğŸ”¢ **7. Hybrid Scoring System**

Combines ML predictions + rule-based behavior:

### Final score:

```
fraud_score = 0.6 * ML_prob + 0.4 * rule_score
```

This creates a powerful numeric ranking of suspicious claims.

---

## ğŸ… **8. Ranking Top Suspicious Claims**

Created a dataframe:

| fraud_score | ml_prob | rule_score | iso_anomaly_score | actual_class |

Then sorted descending:

```python
fraud_results_sorted = fraud_results.sort_values("fraud_score", ascending=False)
```

Exported:

* **Top 50 risky transactions**
* **Full fraud dashboard CSV**

---

## ğŸ“ˆ **9. Tableau Dashboard**

Built using `dashboard/fraud_dashboard_data.csv`.

Contains:

### ğŸ”¹ Heatmap

`fraud_score` vs `Amount` / `Time`

### ğŸ”¹ Suspicious Transactions Table

Ranked by fraud score

### ğŸ”¹ Fraud vs Non-Fraud Segments

Compare distributions
Identify high-risk patterns

### ğŸ”¹ Anomaly Map

IsolationForestâ€™s outliers highlighted

This visually explains fraud behavior and helps business teams act fast.

---

## âš™ï¸ **Tech Stack**

* Python (pandas, numpy)
* scikit-learn
* imbalanced-learn (SMOTE)
* Isolation Forest
* Tableau (Dashboarding)
* Jupyter Notebook
* GitHub

---

## ğŸš€ **How to Run (Ubuntu + Miniconda)**

### 1ï¸âƒ£ Create Conda environment

```bash
conda create -n fraud_env python=3.10 -y
conda activate fraud_env
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Place dataset

Download from Kaggle and place in:

```
data/creditcard.csv
```

### 4ï¸âƒ£ Run notebooks in order

1. `01_fraud_eda_cleaning.ipynb`
2. `02_fraud_modeling.ipynb`
3. `03_fraud_tableau_export.ipynb`

---

## ğŸ“ **Results**

* Built a complete fraud detection pipeline
* Handled severe class imbalance
* Combined supervised + unsupervised models
* Created hybrid fraud scoring
* Produced business-friendly Tableau dashboard
* Fully documented in GitHub-friendly structure

---

## ğŸ™Œ **Author**

Deepali Sharma
https://www.linkedin.com/in/deepali007



Just tell me!
